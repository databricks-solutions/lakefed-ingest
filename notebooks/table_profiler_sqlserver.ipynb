{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e56825-bb39-4607-9b73-f3c237655cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Table Partition Column Profiler \n",
    "\n",
    "\n",
    "\n",
    "This notebook helps identify the best partition column for ingesting large tables from SQL Server into Databricks using the Lakefed Ingest tool.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- `USE CONNECTION` privilege on the foreign catalog connection\n",
    "- `SELECT` privilege on the source SQL Server tables\n",
    "- Access to a compute cluster with DBR 17.3+\n",
    "- Foreign catalog configured for SQL Server connection\n",
    "\n",
    "#### **Requirements for `remote_query()`:**\n",
    "\n",
    "- Unity Catalog enabled workspace\n",
    "\n",
    "- Network connectivity from your Databricks Runtime cluster or SQL warehouse to the target database systems. [See Networking recommendations for Lakehouse Federation.](https://docs.databricks.com/aws/en/query-federation/networking)\n",
    "- Databricks clusters must use Databricks Runtime 17.3 or above.\n",
    "\n",
    "**Permissions required:**\n",
    "\n",
    "- To use the remote_query function, you must have the `USE CONNECTION` privilege on the connection or the `SELECT` privilege on a view that wraps the function. \n",
    "- Single-user clusters also require the `MANAGE` permission on the connection.\n",
    "\n",
    "> Note - `remote_query()` vs Federation - `remote_query()` pushes computation to SQL Server (preferred for large tables)\n",
    " > whereas, direct federation through foreign catalogs pulls data to Databricks.\n",
    "> This notebook provides both options - use remote_query when available. Refer documentation [here](https://docs.databricks.com/aws/en/query-federation/remote-queries).\n",
    "\n",
    "**Key considerations:**\n",
    "- having a clustered index on the cluster/partition column on the sql server side is mandatory for ingestion performance\n",
    "- Primary keys and unique indexes are poor partition/cluster columns (excluded automatically)\n",
    "- Only integer columns are considered (timestamps/floats cause issues)\n",
    "- Tables >10TB should use sampling mode for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659ad558-640f-42d8-b4c4-bff400528c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n  Mode: remote_query\n  Catalog: sqlserver_edwia_catalog\n  Table: dbo.store_sales_1tb\n  Sampling: Disabled\n"
     ]
    }
   ],
   "source": [
    "# Widget parameters for easy configuration\n",
    "dbutils.widgets.text(\"connection_name\", \"\", \"Connection Name (for remote_query - leave empty to use federation)\")\n",
    "dbutils.widgets.text(\"sql_database\", \"\", \"SQL Server Database (for remote_query)\")\n",
    "dbutils.widgets.text(\"src_catalog\", \"sqlserver_catalog\", \"UC Foreign Catalog\")\n",
    "dbutils.widgets.text(\"src_schema\", \"dbo\", \"Source Schema\") \n",
    "dbutils.widgets.text(\"src_table\", \"store_sales_1tb\", \"Source Table (REQUIRED)\")\n",
    "dbutils.widgets.dropdown(\"use_sampling\", \"false\", [\"true\", \"false\"], \"Use Sampling (for 10TB+ tables)\")\n",
    "dbutils.widgets.text(\"table_size_gb\", \"\", \"Table Size in GB (REQUIRED)\")\n",
    "\n",
    "# Get values\n",
    "connection_name = dbutils.widgets.get(\"connection_name\")\n",
    "sql_database = dbutils.widgets.get(\"sql_database\")\n",
    "src_catalog = dbutils.widgets.get(\"src_catalog\")\n",
    "src_schema = dbutils.widgets.get(\"src_schema\")\n",
    "src_table = dbutils.widgets.get(\"src_table\")\n",
    "use_sampling = dbutils.widgets.get(\"use_sampling\") == \"true\"\n",
    "table_size_gb = dbutils.widgets.get(\"table_size_gb\")\n",
    "\n",
    "\n",
    "# Validate required fields\n",
    "if not src_table:\n",
    "    raise ValueError(\"Source table is required!\")\n",
    "if not table_size_gb:\n",
    "    raise ValueError(\"Table size is required!\")\n",
    "\n",
    "try:\n",
    "    table_size_gb = float(table_size_gb)\n",
    "except ValueError:\n",
    "    raise ValueError(\"Table size must be a number!\")\n",
    "\n",
    "# Auto-enable sampling for very large tables\n",
    "if table_size_gb > 10000 and not use_sampling:\n",
    "    print(f\"⚠️ WARNING: Table is {table_size_gb:.0f}GB - consider enabling sampling!\")\n",
    "# Determine query mode\n",
    "USE_REMOTE_QUERY = bool(connection_name and sql_database)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Mode: {'remote_query' if USE_REMOTE_QUERY else 'Federation (fallback)'}\")\n",
    "print(f\"  Catalog: {src_catalog}\")\n",
    "print(f\"  Table: {src_schema}.{src_table}\")\n",
    "print(f\"  Sampling: {'Enabled' if use_sampling else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1ff425-f00e-4db2-b812-0ac324a6493a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Identify Candidate Columns\n",
    "First, we identify indexed columns that could be good partition candidates. We automatically exclude primary keys and unique indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c33b1fb-247e-4778-9063-f172c2bbcf4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for primary keys on dbo.store_sales_1tb...\n\n✓ Found 2 candidate columns: ss_item_sk, ss_sold_date_sk\n"
     ]
    }
   ],
   "source": [
    "# Find Indexed Columns (Excluding Keys)\n",
    "\n",
    "# Get primary key columns to exclude\n",
    "pk_query = f\"\"\"\n",
    "SELECT \n",
    "    kc.column_name,\n",
    "    'PRIMARY KEY' as constraint_type\n",
    "FROM {src_catalog}.information_schema.key_column_usage kc\n",
    "JOIN {src_catalog}.information_schema.table_constraints tc\n",
    "    ON kc.constraint_name = tc.constraint_name\n",
    "    AND kc.table_name = tc.table_name\n",
    "    AND kc.table_schema = tc.table_schema\n",
    "WHERE tc.constraint_type = 'PRIMARY KEY'\n",
    "    AND kc.table_schema = '{src_schema}'\n",
    "    AND kc.table_name = '{src_table}'\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Checking for primary keys on {src_schema}.{src_table}...\")\n",
    "\n",
    "pk_columns = [row['column_name'] for row in spark.sql(pk_query).collect()]\n",
    "\n",
    "if pk_columns:\n",
    "    print(f\"⚠️ Excluding primary key columns: {', '.join(pk_columns)}\")\n",
    "\n",
    "# Get indexed integer columns (best partition candidates)\n",
    "index_query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    c.name as column_name,\n",
    "    i.type_desc as index_type\n",
    "FROM {src_catalog}.sys.indexes i\n",
    "INNER JOIN {src_catalog}.sys.index_columns ic \n",
    "    ON i.object_id = ic.object_id AND i.index_id = ic.index_id\n",
    "INNER JOIN {src_catalog}.sys.columns c \n",
    "    ON ic.object_id = c.object_id AND ic.column_id = c.column_id\n",
    "INNER JOIN {src_catalog}.sys.tables t \n",
    "    ON i.object_id = t.object_id\n",
    "INNER JOIN {src_catalog}.sys.schemas s \n",
    "    ON t.schema_id = s.schema_id\n",
    "WHERE s.name = '{src_schema}' \n",
    "    AND t.name = '{src_table}'\n",
    "    AND i.type > 0\n",
    "    AND NOT ic.is_included_column\n",
    "    AND c.system_type_id IN (48, 52, 56, 127)  -- integer types only\n",
    "    AND NOT i.is_primary_key\n",
    "    AND NOT i.is_unique\n",
    "ORDER BY index_type DESC\n",
    "\"\"\"\n",
    "\n",
    "candidate_columns = [row['column_name'] for row in spark.sql(index_query).collect()]\n",
    "\n",
    "print(f\"\\n✓ Found {len(candidate_columns)} candidate columns: {', '.join(candidate_columns[:10])}\")\n",
    "\n",
    "if not candidate_columns:\n",
    "    print(\"\\n⚠️ No indexed columns found. Checking all integer columns...\")\n",
    "    fallback_query = f\"\"\"\n",
    "    SELECT column_name \n",
    "    FROM {src_catalog}.information_schema.columns\n",
    "    WHERE table_schema = '{src_schema}'\n",
    "        AND table_name = '{src_table}'\n",
    "        AND data_type IN ('int', 'bigint', 'smallint', 'tinyint')\n",
    "        AND column_name NOT IN ({','.join([f\"'{c}'\" for c in pk_columns]) if pk_columns else \"''\"})\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    candidate_columns = [row['column_name'] for row in spark.sql(fallback_query).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042d6a15-cf8e-4a00-85b8-2b2fdd547901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Profile Candidate Columns\n",
    "Now we analyze each candidate column for distribution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb86d154-9dd8-40dd-b118-b470f18d811e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Profiling ss_item_sk... ✓ Distinct: 402,000, Skew: 2.1x\n[2/2] Profiling ss_sold_date_sk... ✓ Distinct: 1,827, Skew: 21.6x\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>column_name</th><th>total_rows</th><th>distinct_values</th><th>null_percentage</th><th>min_value</th><th>max_value</th><th>avg_rows_per_value</th><th>max_rows_per_value</th></tr></thead><tbody><tr><td>ss_item_sk</td><td>7031221350</td><td>402000</td><td>0.0</td><td>1</td><td>402000</td><td>17490</td><td>36780</td></tr><tr><td>ss_sold_date_sk</td><td>7031221350</td><td>1827</td><td>0.0</td><td>2450816</td><td>2452642</td><td>3848506</td><td>83202375</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ss_item_sk",
         7031221350,
         402000,
         0.0,
         1,
         402000,
         17490,
         36780
        ],
        [
         "ss_sold_date_sk",
         7031221350,
         1827,
         0.0,
         2450816,
         2452642,
         3848506,
         83202375
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "column_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "distinct_values",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "null_percentage",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "min_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "max_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_rows_per_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "max_rows_per_value",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Helper function for SQL Server identifier escaping\n",
    "def tsql_ident(name: str) -> str:\n",
    "    \"\"\"Escape SQL Server identifiers\"\"\"\n",
    "    return '[' + name.replace(']', ']]') + ']'\n",
    "\n",
    "# Query templates\n",
    "QUERIES = {\n",
    "    'basic_stats': \"\"\"\n",
    "        SELECT \n",
    "            COUNT_BIG(*) AS total_rows,\n",
    "            COUNT_BIG(DISTINCT {column}) AS distinct_values,\n",
    "            COUNT_BIG({column}) AS non_null_count,\n",
    "            MIN({column}) AS min_value,\n",
    "            MAX({column}) AS max_value\n",
    "        FROM {schema}.{table}\n",
    "    \"\"\",\n",
    "    \n",
    "    'distribution_stats': \"\"\"\n",
    "        SELECT \n",
    "            AVG(CAST(freq AS FLOAT)) AS avg_frequency,\n",
    "            MAX(freq) AS max_frequency,\n",
    "            MIN(freq) AS min_frequency,\n",
    "            STDEV(freq) AS stddev_frequency\n",
    "        FROM (\n",
    "            SELECT COUNT_BIG(*) AS freq\n",
    "            FROM {schema}.{table}\n",
    "            WHERE {column} IS NOT NULL\n",
    "            GROUP BY {column}\n",
    "        ) AS frequency_table\n",
    "    \"\"\",\n",
    "\n",
    "    'distribution_stats_sampled': \"\"\"\n",
    "    SELECT \n",
    "        AVG(CAST(freq AS FLOAT)) AS avg_frequency,\n",
    "        MAX(freq) AS max_frequency,\n",
    "        MIN(freq) AS min_frequency,\n",
    "        STDEV(freq) AS stddev_frequency\n",
    "    FROM (\n",
    "        SELECT {column}, COUNT(*) AS freq\n",
    "        FROM (\n",
    "            SELECT TOP 10000000 {column}\n",
    "            FROM {schema}.{table} TABLESAMPLE (1 PERCENT)\n",
    "            WHERE {column} IS NOT NULL\n",
    "        ) AS sampled\n",
    "        GROUP BY {column}\n",
    "    ) AS frequency_table\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "def profile_with_remote_query(column, connection, database):\n",
    "    \"\"\"Profile using remote_query (computation on SQL Server)\"\"\"\n",
    "    safe_column = tsql_ident(column)\n",
    "    safe_schema = tsql_ident(src_schema)\n",
    "    safe_table = tsql_ident(src_table)\n",
    "    \n",
    "    # Use the template from QUERIES dict\n",
    "    query = QUERIES['basic_stats'].format(\n",
    "        column=safe_column,\n",
    "        schema=safe_schema,\n",
    "        table=safe_table\n",
    "    )\n",
    "    escaped_query = query.replace(\"'\", \"''\")\n",
    "    \n",
    "    stats_sql = f\"\"\"\n",
    "    SELECT * FROM remote_query(\n",
    "        '{connection}',\n",
    "        database => '{database}',\n",
    "        query => '{escaped_query}',\n",
    "        fetchsize => '10'\n",
    "    )\n",
    "    \"\"\"\n",
    "    stats = spark.sql(stats_sql).collect()[0]\n",
    "    \n",
    "    # Use appropriate distribution template\n",
    "    dist_template = 'distribution_stats_sampled' if use_sampling else 'distribution_stats'\n",
    "    dist_query = QUERIES[dist_template].format(\n",
    "        column=safe_column,\n",
    "        schema=safe_schema,\n",
    "        table=safe_table\n",
    "    )\n",
    "    escaped_dist = dist_query.replace(\"'\", \"''\")\n",
    "    \n",
    "    dist_sql = f\"\"\"\n",
    "    SELECT * FROM remote_query(\n",
    "        '{connection}',\n",
    "        database => '{database}',\n",
    "        query => '{escaped_dist}',\n",
    "        fetchsize => '10'\n",
    "    )\n",
    "    \"\"\"\n",
    "    dist = spark.sql(dist_sql).collect()[0]\n",
    "    \n",
    "    return stats, dist\n",
    "\n",
    "def profile_with_federation(column):\n",
    "    \"\"\"Profile using direct federation (fallback)\"\"\"\n",
    "    # Basic stats\n",
    "    stats = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(DISTINCT {column}) as distinct_values,\n",
    "            COUNT({column}) as non_null_count,\n",
    "            MIN({column}) as min_value,\n",
    "            MAX({column}) as max_value\n",
    "        FROM {src_catalog}.{src_schema}.{src_table}\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    # Distribution stats (with sampling for large tables)\n",
    "    if use_sampling:\n",
    "        dist = spark.sql(f\"\"\"\n",
    "            WITH sampled AS (\n",
    "                SELECT {column}\n",
    "                FROM {src_catalog}.{src_schema}.{src_table}\n",
    "                WHERE {column} IS NOT NULL\n",
    "                LIMIT 10000000\n",
    "            )\n",
    "            SELECT \n",
    "                AVG(cnt) as avg_frequency,\n",
    "                MAX(cnt) as max_frequency\n",
    "            FROM (\n",
    "                SELECT COUNT(*) as cnt\n",
    "                FROM sampled\n",
    "                GROUP BY {column}\n",
    "            ) freq\n",
    "        \"\"\").collect()[0]\n",
    "    else:\n",
    "        dist = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                AVG(cnt) as avg_frequency,\n",
    "                MAX(cnt) as max_frequency\n",
    "            FROM (\n",
    "                SELECT COUNT(*) as cnt\n",
    "                FROM {src_catalog}.{src_schema}.{src_table}\n",
    "                WHERE {column} IS NOT NULL\n",
    "                GROUP BY {column}\n",
    "            ) freq\n",
    "        \"\"\").collect()[0]\n",
    "    \n",
    "    return stats, dist\n",
    "\n",
    "# Profile each candidate\n",
    "results = []\n",
    "\n",
    "for i, column in enumerate(candidate_columns[:10], 1):  # Limit to top 10\n",
    "    print(f\"[{i}/{min(10, len(candidate_columns))}] Profiling {column}... \", end=\"\")\n",
    "    \n",
    "    try:\n",
    "        if USE_REMOTE_QUERY:\n",
    "            stats, dist = profile_with_remote_query(column, connection_name, sql_database)\n",
    "        else:\n",
    "            stats, dist = profile_with_federation(column)\n",
    "        \n",
    "        total_rows = int(stats['total_rows'])\n",
    "        distinct_values = int(stats['distinct_values'])\n",
    "        null_count = total_rows - int(stats['non_null_count'])\n",
    "        \n",
    "        result = {\n",
    "            'column_name': column,\n",
    "            'total_rows': total_rows,\n",
    "            'distinct_values': distinct_values,\n",
    "            'null_percentage': round(null_count * 100.0 / total_rows, 2) if total_rows > 0 else 0,\n",
    "            'min_value': stats['min_value'],\n",
    "            'max_value': stats['max_value'],\n",
    "            'avg_rows_per_value': int(dist['avg_frequency']) if dist['avg_frequency'] else 0,\n",
    "            'max_rows_per_value': int(dist['max_frequency']) if dist['max_frequency'] else 0\n",
    "        }\n",
    "        \n",
    "        # Calculate skew for display\n",
    "        skew = result['max_rows_per_value'] / result['avg_rows_per_value'] if result['avg_rows_per_value'] > 0 else 999\n",
    "        print(f\"✓ Distinct: {distinct_values:,}, Skew: {skew:.1f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {str(e)[:50]}\")\n",
    "        result = {'column_name': column, 'error': str(e)[:50]}\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e44253-b0ec-476e-a2ff-10bc91b907f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Score and Rank Columns\n",
    "Score each column based on its suitability as a partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a28495-141a-4b4a-aa55-c7c5c294ab27",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764145591061}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>column_name</th><th>distinct_values</th><th>null_percentage</th><th>avg_rows_per_value</th><th>max_rows_per_value</th><th>skew_ratio</th><th>partition_score</th><th>recommendation</th></tr></thead><tbody><tr><td>ss_item_sk</td><td>402000</td><td>0.0</td><td>17490</td><td>36780</td><td>2.1029159519725558</td><td>50</td><td> ACCEPTABLE</td></tr><tr><td>ss_sold_date_sk</td><td>1827</td><td>0.0</td><td>3848506</td><td>83202375</td><td>21.619395942217576</td><td>-5</td><td> Extreme skew (22x)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ss_item_sk",
         402000,
         0.0,
         17490,
         36780,
         2.1029159519725558,
         50,
         " ACCEPTABLE"
        ],
        [
         "ss_sold_date_sk",
         1827,
         0.0,
         3848506,
         83202375,
         21.619395942217576,
         -5,
         " Extreme skew (22x)"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "column_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "distinct_values",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "null_percentage",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_rows_per_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "max_rows_per_value",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "skew_ratio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "partition_score",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "recommendation",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n⚠️ IMPORTANT: Please consult with your Databricks Solution Architect or\n   SME team to validate the partition column selection,\n   especially for production workloads over 10TB.\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_partition_score(row):\n",
    "    \"\"\"Score column suitability for partitioning (0-100)\"\"\"\n",
    "    if 'error' in row or not row.get('distinct_values'):\n",
    "        return -1\n",
    "    \n",
    "    total_rows = row['total_rows']\n",
    "    distinct = row['distinct_values']\n",
    "    null_pct = row['null_percentage']\n",
    "    avg_rows = row['avg_rows_per_value']\n",
    "    max_rows = row['max_rows_per_value']\n",
    "    \n",
    "    # HARD DISQUALIFIERS\n",
    "    \n",
    "    # Nulls > 1% - REJECT\n",
    "    if null_pct > 1:\n",
    "        return -4\n",
    "    \n",
    "    # Calculate skew ratio\n",
    "    skew_ratio = max_rows / avg_rows if avg_rows > 0 else 999\n",
    "    \n",
    "    # Extreme skew (>10x) \n",
    "    if skew_ratio > 10:\n",
    "        return -5\n",
    "    \n",
    "    # Too unique (>50% distinct) \n",
    "    cardinality_ratio = distinct / total_rows if total_rows > 0 else 1\n",
    "    if cardinality_ratio > 0.5:\n",
    "        return -1\n",
    "    \n",
    "    # Too few distinct values (<100) \n",
    "    if distinct < 100:\n",
    "        return -2\n",
    "    \n",
    "    # SCORING FOR VALID CANDIDATES\n",
    "    score = 100\n",
    "    \n",
    "    # Heavy penalty for skew (this is critical!)\n",
    "    if skew_ratio > 5:\n",
    "        score -= 60  # 5-10x skew\n",
    "    elif skew_ratio > 3:\n",
    "        score -= 40  # 3-5x skew\n",
    "    elif skew_ratio > 2:\n",
    "        score -= 20  # 2-3x skew\n",
    "    \n",
    "    # Penalty for any nulls\n",
    "    if null_pct > 0.5:\n",
    "        score -= 20\n",
    "    elif null_pct > 0:\n",
    "        score -= 10\n",
    "    \n",
    "    # Cardinality penalties\n",
    "    if distinct > 100000:\n",
    "        score -= 30  # Too many partitions\n",
    "    elif distinct < 500:\n",
    "        score -= 20  # Too few partitions\n",
    "    \n",
    "    return max(0, score)\n",
    "\n",
    "def get_recommendation(row):\n",
    "    score = row.get('partition_score', -1)\n",
    "    skew = row.get('max_rows_per_value', 0) / row.get('avg_rows_per_value', 1) if row.get('avg_rows_per_value', 0) > 0 else 999\n",
    "    \n",
    "    if score == -5:\n",
    "        return f' Extreme skew ({skew:.0f}x)'\n",
    "    elif score == -4:\n",
    "        return f' Too many NULLs ({row.get(\"null_percentage\", 0):.1f}%)'\n",
    "    elif score == -1:\n",
    "        return ' Too unique'\n",
    "    elif score == -2:\n",
    "        return ' Too few distinct values'\n",
    "    elif score >= 70:\n",
    "        return ' EXCELLENT'\n",
    "    elif score >= 40:\n",
    "        return ' ACCEPTABLE'\n",
    "    else:\n",
    "        return ' POOR - High skew or other issues'\n",
    "\n",
    "# Apply scoring\n",
    "results_df['partition_score'] = results_df.apply(calculate_partition_score, axis=1)\n",
    "results_df['recommendation'] = results_df.apply(get_recommendation, axis=1)\n",
    "\n",
    "# Sort by score\n",
    "results_df = results_df.sort_values('partition_score', ascending=False)\n",
    "\n",
    "# Display with skew ratio visible\n",
    "results_df['skew_ratio'] = results_df['max_rows_per_value'] / results_df['avg_rows_per_value']\n",
    "display(results_df[['column_name', 'distinct_values', 'null_percentage', \n",
    "                    'avg_rows_per_value', 'max_rows_per_value', 'skew_ratio',\n",
    "                    'partition_score', 'recommendation']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"⚠️ IMPORTANT: Please consult with your Databricks Solution Architect or\")\n",
    "print(\"   SME team to validate the partition column selection,\") \n",
    "print(\"   especially for production workloads over 10TB.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b00afb5e-2ac4-4a09-bff9-82f27ed13320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Final Recommendations\n",
    "Generate configuration recommendations for the best partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9033549f-2032-4f69-8a97-0946b111efea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nRECOMMENDED CONFIGURATION\n============================================================\n\nBest Partition Column: ss_item_sk\nScore: 50/100\nDistribution: 402,000 distinct values\nSkew: 2.1x (max vs avg)\nAvg rows per value: 17,490\nMax rows per value: 36,780\n\n\uD83D\uDCCA PARTITION SIZING ANALYSIS\nTable size: 1134.0 GB\nTarget partitions: ~2000\nRecommended partition_size_mb: 1024\n\n\uD83D\uDCCA PARTITION SIZING ANALYSIS\nEstimated table size: 3274.2 GB\nAverage MB per ss_item_sk: 8.3 MB\nRecommended partition_size_mb: 1024\nEstimated total partitions: 3274\n\nControl Table Configuration:\n----------------------------\nUPDATE sqlserver_edwia_catalog.control_table\nSET \n    partition_col = 'ss_item_sk',\n    partition_size_mb = 1024,\n    load_partitioned = true\nWHERE src_table = 'store_sales_1tb';\n\n"
     ]
    }
   ],
   "source": [
    "valid_columns = results_df[results_df['partition_score'] > 0]\n",
    "\n",
    "if not valid_columns.empty:\n",
    "    best = valid_columns.iloc[0]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"RECOMMENDED CONFIGURATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nBest Partition Column: {best['column_name']}\")\n",
    "    print(f\"Score: {best['partition_score']:.0f}/100\")\n",
    "    print(f\"Distribution: {best['distinct_values']:,} distinct values\")\n",
    "    \n",
    "    skew_ratio = best['max_rows_per_value'] / best['avg_rows_per_value'] if best['avg_rows_per_value'] > 0 else 0\n",
    "    print(f\"Skew: {skew_ratio:.1f}x (max vs avg)\")\n",
    "    print(f\"Avg rows per value: {best['avg_rows_per_value']:,}\")\n",
    "    print(f\"Max rows per value: {best['max_rows_per_value']:,}\")\n",
    "    \n",
    "    # Use actual table size provided by user\n",
    "    table_size_mb = table_size_gb * 1024\n",
    "    total_rows = best['total_rows']\n",
    "    distinct_values = best['distinct_values']\n",
    "\n",
    "    # Target partition counts based on actual size\n",
    "    if table_size_gb < 100:  # <100GB\n",
    "        target_partitions = 100\n",
    "    elif table_size_gb < 1000:  # 100GB-1TB\n",
    "        target_partitions = 500\n",
    "    elif table_size_gb < 10000:  # 1-10TB\n",
    "        target_partitions = 2000\n",
    "    else:  # >10TB\n",
    "        target_partitions = 5000\n",
    "\n",
    "    # Calculate ideal partition size\n",
    "    ideal_partition_mb = int(table_size_mb / target_partitions)\n",
    "\n",
    "    # Round to standard sizes\n",
    "    if ideal_partition_mb < 512:\n",
    "        partition_mb = 512\n",
    "    elif ideal_partition_mb < 1024:\n",
    "        partition_mb = 1024\n",
    "    elif ideal_partition_mb < 2048:\n",
    "        partition_mb = 2048\n",
    "    elif ideal_partition_mb < 4096:\n",
    "        partition_mb = 4096\n",
    "    else:\n",
    "        partition_mb = 8192\n",
    "\n",
    "    print(f\"\\n\uD83D\uDCCA PARTITION SIZING ANALYSIS\")\n",
    "    print(f\"Table size: {table_size_gb:.1f} GB\")\n",
    "    print(f\"Target partitions: ~{target_partitions}\")\n",
    "    print(f\"Recommended partition_size_mb: {partition_mb}\")\n",
    "    \n",
    "    # Estimate number of partitions this will create\n",
    "    estimated_partitions = (avg_mb_per_value * distinct_values) / partition_mb\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCCA PARTITION SIZING ANALYSIS\")\n",
    "    print(f\"Estimated table size: {estimated_table_size_mb/1024:.1f} GB\")\n",
    "    print(f\"Average MB per {best['column_name']}: {avg_mb_per_value:.1f} MB\")\n",
    "    print(f\"Recommended partition_size_mb: {partition_mb}\")\n",
    "    print(f\"Estimated total partitions: {estimated_partitions:.0f}\")\n",
    "    \n",
    "    # Warnings\n",
    "    if estimated_partitions > 10000:\n",
    "        print(f\"\\n⚠️ WARNING: This will create {estimated_partitions:.0f} partitions!\")\n",
    "        print(\"Consider using a larger partition_size_mb or different column\")\n",
    "    \n",
    "    if skew_ratio > 10:\n",
    "        print(f\"\\n⚠️ WARNING: High skew detected ({skew_ratio:.1f}x)\")\n",
    "        print(\"Some partitions will be much larger than others\")\n",
    "        print(f\"Largest partition may be ~{avg_mb_per_value * skew_ratio:.0f} MB\")\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Control Table Configuration:\n",
    "----------------------------\n",
    "UPDATE {src_catalog}.control_table\n",
    "SET \n",
    "    partition_col = '{best['column_name']}',\n",
    "    partition_size_mb = {partition_mb},\n",
    "    load_partitioned = true\n",
    "WHERE src_table = '{src_table}';\n",
    "\"\"\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No suitable partition columns found!\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\" Use non-partitioned ingestion:\")\n",
    "    print(f\"\"\"\n",
    "UPDATE {src_catalog}.control_table\n",
    "SET \n",
    "    load_partitioned = false,\n",
    "    partition_col = NULL,\n",
    "    partition_size_mb = NULL\n",
    "WHERE src_table = '{src_table}';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "531020de-f6e5-441f-84c9-7a144257ab1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "table_profiler_sqlserver",
   "widgets": {
    "connection_name": {
     "currentValue": "sqlserver_edwia",
     "nuid": "340768eb-6e60-4ac6-85a6-f215d8f7df0d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Connection Name (for remote_query - leave empty to use federation)",
      "name": "connection_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Connection Name (for remote_query - leave empty to use federation)",
      "name": "connection_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sql_database": {
     "currentValue": "DatabricksTestDB",
     "nuid": "65162289-356f-4952-a8d9-8284590d70bc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "SQL Server Database (for remote_query)",
      "name": "sql_database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "SQL Server Database (for remote_query)",
      "name": "sql_database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_catalog": {
     "currentValue": "sqlserver_edwia_catalog",
     "nuid": "361315fc-07cc-4324-a133-991dd63e357e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sqlserver_catalog",
      "label": "UC Foreign Catalog",
      "name": "src_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sqlserver_catalog",
      "label": "UC Foreign Catalog",
      "name": "src_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_schema": {
     "currentValue": "dbo",
     "nuid": "ab2bdb88-8ab5-4e92-81f4-d3db0ccf5b7b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbo",
      "label": "Source Schema",
      "name": "src_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbo",
      "label": "Source Schema",
      "name": "src_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_table": {
     "currentValue": "store_sales_1tb",
     "nuid": "b7fa467f-39ad-44a2-9035-23e8c95634ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "store_sales_1tb",
      "label": "Source Table (REQUIRED)",
      "name": "src_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "store_sales_1tb",
      "label": "Source Table (REQUIRED)",
      "name": "src_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_size_gb": {
     "currentValue": "1134",
     "nuid": "b33b54ea-3a46-4278-b7d5-9a5801d61b34",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Table Size in GB (REQUIRED)",
      "name": "table_size_gb",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Table Size in GB (REQUIRED)",
      "name": "table_size_gb",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "use_sampling": {
     "currentValue": "false",
     "nuid": "fd6ea3e7-5a0f-4a73-a9c2-66052f618960",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Use Sampling (for 10TB+ tables)",
      "name": "use_sampling",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Use Sampling (for 10TB+ tables)",
      "name": "use_sampling",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}