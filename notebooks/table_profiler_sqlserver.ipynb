{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e56825-bb39-4607-9b73-f3c237655cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Table Partition Column Profiler \n",
    "This notebook helps identify the best partition column for ingesting large tables from SQL Server into Databricks using the Lakefed Ingest tool.\n",
    "\n",
    "### Prerequisites\n",
    "- `USE CONNECTION` privilege on the foreign catalog connection\n",
    "- `SELECT` privilege on the source SQL Server tables\n",
    "- Access to a compute cluster with DBR 17.3+ to take advantate of `remote_query()`. Can fall back to Lakehouse Federation if not available.\n",
    "- Foreign catalog configured for SQL Server connection\n",
    "\n",
    "#### Requirements for `remote_query()`:\n",
    "- Unity Catalog enabled workspace\n",
    "- Network connectivity from your Databricks Runtime cluster or SQL warehouse to the target database systems. [See Networking recommendations for Lakehouse Federation.](https://docs.databricks.com/aws/en/query-federation/networking)\n",
    "- Databricks clusters must use Databricks Runtime 17.3 or above.\n",
    "\n",
    "**Permissions required:**\n",
    "- To use the remote_query function, you must have the `USE CONNECTION` privilege on the connection or the `SELECT` privilege on a view that wraps the function. \n",
    "- Single-user clusters also require the `MANAGE` permission on the connection.\n",
    "\n",
    "> Note - `remote_query()` vs Federation - `remote_query()` pushes down the entire query to SQL Server (preferred for large tables). Federated queries are not always pushed down and may be slower.  \n",
    "> This notebook provides both options - use remote_query when available. Refer documentation [here](https://docs.databricks.com/aws/en/query-federation/remote-queries).\n",
    "\n",
    "**Key considerations:**\n",
    "- Selecting a partition column with an index on the SQL Server side is necessary in order to achieve reasonable performance.\n",
    "- Primary Keys with clustered indexes are IDEAL partition columns (Great distribution / no skew). This profiler is most useful for tables without a clustered primary key.\n",
    "- Only integer columns are considered (timestamps/floats cause issues).\n",
    "- Tables >10TB should use sampling mode for performance.\n",
    "- Default partition size of 2048MB is recommended for partitioned ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659ad558-640f-42d8-b4c4-bff400528c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get values\n",
    "connection_name = dbutils.widgets.get(\"connection_name\")\n",
    "sql_database = dbutils.widgets.get(\"sql_database\")\n",
    "src_catalog = dbutils.widgets.get(\"src_catalog\")\n",
    "src_schema = dbutils.widgets.get(\"src_schema\")\n",
    "src_table = dbutils.widgets.get(\"src_table\")\n",
    "use_sampling = True if dbutils.widgets.get(\"use_sampling\") == \"true\" else False\n",
    "\n",
    "\n",
    "# Validate required fields\n",
    "if not src_table:\n",
    "    raise ValueError(\"Source table is required!\")\n",
    "\n",
    "# Determine query mode\n",
    "USE_REMOTE_QUERY = bool(connection_name and sql_database)\n",
    "\n",
    "print(f\"Mode: {'remote_query' if USE_REMOTE_QUERY else 'Federation'}\")\n",
    "print(f\"Table: {src_catalog}.{src_schema}.{src_table}\")\n",
    "print(f\"Sampling: {'Enabled (10TB+ mode)' if use_sampling else 'Disabled'}\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Mode: {'remote_query' if USE_REMOTE_QUERY else 'Federation (fallback)'}\")\n",
    "print(f\"  Catalog: {src_catalog}\")\n",
    "print(f\"  Table: {src_schema}.{src_table}\")\n",
    "print(f\"  Sampling: {'Enabled' if use_sampling else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1ff425-f00e-4db2-b812-0ac324a6493a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Identify Candidate Columns\n",
    "First, we identify indexed columns that could be good partition candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c33b1fb-247e-4778-9063-f172c2bbcf4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FOUND_CLUSTERED_PK = None\n",
    "\n",
    "# Check for clustered PK\n",
    "pk_query = f\"\"\"\n",
    "SELECT \n",
    "    c.name as column_name,\n",
    "    i.type_desc as index_type,\n",
    "    t.name as data_type\n",
    "FROM {src_catalog}.sys.indexes i\n",
    "INNER JOIN {src_catalog}.sys.index_columns ic \n",
    "    ON i.object_id = ic.object_id AND i.index_id = ic.index_id\n",
    "INNER JOIN {src_catalog}.sys.columns c\n",
    "    ON ic.object_id = c.object_id AND ic.column_id = c.column_id\n",
    "INNER JOIN {src_catalog}.sys.types t\n",
    "    ON c.system_type_id = t.system_type_id\n",
    "INNER JOIN {src_catalog}.sys.tables tab\n",
    "    ON i.object_id = tab.object_id\n",
    "INNER JOIN {src_catalog}.sys.schemas s\n",
    "    ON tab.schema_id = s.schema_id\n",
    "WHERE i.is_primary_key\n",
    "    AND i.type_desc = 'CLUSTERED'\n",
    "    AND s.name = '{src_schema}'\n",
    "    AND tab.name = '{src_table}'\n",
    "    AND t.name IN ('int', 'bigint', 'smallint', 'tinyint')\n",
    "\"\"\"\n",
    "\n",
    "clustered_pk_df = spark.sql(pk_query)\n",
    "display(clustered_pk_df)\n",
    "clustered_pk = clustered_pk_df.collect()\n",
    "\n",
    "if clustered_pk:\n",
    "    if len(clustered_pk) == 1:\n",
    "        pk_column = clustered_pk[0]['column_name']\n",
    "        print(\"=\" * 60)\n",
    "        print(\"✅ FOUND CLUSTERED PRIMARY KEY - USE THIS!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nColumn: {pk_column}\")\n",
    "        print(\"\\nWhy this is ideal:\")\n",
    "        print(\"  • Great distribution / no skew\")\n",
    "        print(\"  • Index for fast MIN/MAX\")\n",
    "        print(\"\\nRecommended Configuration:\")\n",
    "        print(f\"\"\"\n",
    "        UPDATE control_table\n",
    "        SET \n",
    "            partition_col = '{pk_column}',\n",
    "            partition_size_mb = 2048,\n",
    "            load_partitioned = true\n",
    "        WHERE src_table = '{src_catalog}.{src_schema}.{src_table}';\n",
    "        \"\"\")\n",
    "    else:\n",
    "        # Composite PK\n",
    "        pk_columns = [row['column_name'] for row in clustered_pk]\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"⚠️ COMPOSITE PRIMARY KEY ({len(pk_columns)} columns): {', '.join(pk_columns)}\")\n",
    "        print(\"Cannot use composite key directly for partitioning\")\n",
    "        print(\"\\nRun the next sections - profile individual columns to find best option:\")\n",
    "        print(\"=\" * 60)\n",
    "        pk_column = None\n",
    "    SKIP_PROFILING = False\n",
    "\n",
    "else:\n",
    "    print(\"No clustered primary key found.\")\n",
    "    print(\"Run the next section to analyze alternative columns...\")\n",
    "    SKIP_PROFILING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5a93151-c431-4791-8bde-f0c5a143f272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not SKIP_PROFILING:\n",
    "    # Find indexed integer columns\n",
    "    index_query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        c.name as column_name,\n",
    "        i.type_desc as index_type\n",
    "    FROM {src_catalog}.sys.indexes i\n",
    "    INNER JOIN {src_catalog}.sys.index_columns ic \n",
    "        ON i.object_id = ic.object_id AND i.index_id = ic.index_id\n",
    "    INNER JOIN {src_catalog}.sys.columns c\n",
    "        ON ic.object_id = c.object_id AND ic.column_id = c.column_id\n",
    "    INNER JOIN {src_catalog}.sys.tables t\n",
    "        ON i.object_id = t.object_id\n",
    "    INNER JOIN {src_catalog}.sys.schemas s\n",
    "        ON t.schema_id = s.schema_id\n",
    "    WHERE s.name = '{src_schema}'\n",
    "        AND t.name = '{src_table}'\n",
    "        AND i.type > 0\n",
    "        AND NOT ic.is_included_column\n",
    "        AND c.system_type_id IN (48, 52, 56, 127)  -- Integer types\n",
    "        AND NOT i.is_primary_key  -- Already checked PKs\n",
    "    ORDER BY index_type DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    candidate_columns = [row['column_name'] for row in spark.sql(index_query).collect()]\n",
    "    \n",
    "    if not candidate_columns:\n",
    "        print(\"No indexed columns found. Checking all integer columns...\")\n",
    "        fallback_query = f\"\"\"\n",
    "        SELECT column_name \n",
    "        FROM {src_catalog}.information_schema.columns\n",
    "        WHERE table_schema = '{src_schema}'\n",
    "            AND table_name = '{src_table}'\n",
    "            AND data_type IN ('int', 'bigint', 'smallint', 'tinyint')\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        candidate_columns = [row['column_name'] for row in spark.sql(fallback_query).collect()]\n",
    "    \n",
    "    print(f\"Found {len(candidate_columns)} candidates to analyze: {', '.join(candidate_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042d6a15-cf8e-4a00-85b8-2b2fdd547901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Profile Candidate Columns\n",
    "Now we analyze each candidate column for distribution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb86d154-9dd8-40dd-b118-b470f18d811e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not SKIP_PROFILING and candidate_columns:\n",
    "    \n",
    "    # Helper function\n",
    "    def tsql_ident(name: str) -> str:\n",
    "        \"\"\"Escape SQL Server identifiers\"\"\"\n",
    "        return '[' + name.replace(']', ']]') + ']'\n",
    "    \n",
    "    # Query templates - INCLUDING sampling versions\n",
    "    QUERIES = {\n",
    "        'basic_stats': \"\"\"\n",
    "            SELECT \n",
    "                COUNT_BIG(*) AS total_rows,\n",
    "                COUNT_BIG(DISTINCT {column}) AS distinct_values,\n",
    "                COUNT_BIG({column}) AS non_null_count,\n",
    "                MIN({column}) AS min_value,\n",
    "                MAX({column}) AS max_value\n",
    "            FROM {schema}.{table}\n",
    "        \"\"\",\n",
    "        \n",
    "        'distribution_stats': \"\"\"\n",
    "            SELECT \n",
    "                AVG(CAST(freq AS FLOAT)) AS avg_frequency,\n",
    "                MAX(freq) AS max_frequency\n",
    "            FROM (\n",
    "                SELECT COUNT_BIG(*) AS freq\n",
    "                FROM {schema}.{table}\n",
    "                WHERE {column} IS NOT NULL\n",
    "                GROUP BY {column}\n",
    "            ) AS freq_table\n",
    "        \"\"\",\n",
    "        \n",
    "        'distribution_stats_sampled': \"\"\"\n",
    "            SELECT \n",
    "                AVG(CAST(freq AS FLOAT)) AS avg_frequency,\n",
    "                MAX(freq) AS max_frequency\n",
    "            FROM (\n",
    "                SELECT {column}, COUNT(*) AS freq\n",
    "                FROM (\n",
    "                    SELECT TOP 10000000 {column}\n",
    "                    FROM {schema}.{table} TABLESAMPLE (1 PERCENT)\n",
    "                    WHERE {column} IS NOT NULL\n",
    "                ) AS sampled\n",
    "                GROUP BY {column}\n",
    "            ) AS freq_table\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    def profile_column(column):\n",
    "        \"\"\"Profile a single column - always returns consistent types\"\"\"\n",
    "        safe_column = tsql_ident(column)\n",
    "        safe_schema = tsql_ident(src_schema)\n",
    "        safe_table = tsql_ident(src_table)\n",
    "        \n",
    "        if USE_REMOTE_QUERY:\n",
    "            # Basic stats (always full scan)\n",
    "            basic_query = QUERIES['basic_stats'].format(\n",
    "                column=safe_column, schema=safe_schema, table=safe_table\n",
    "            )\n",
    "            escaped_query = basic_query.replace(\"'\", \"''\")\n",
    "            \n",
    "            stats_sql = f\"\"\"\n",
    "            SELECT * FROM remote_query(\n",
    "                '{connection_name}',\n",
    "                database => '{sql_database}',\n",
    "                query => '{escaped_query}',\n",
    "                fetchsize => '10'\n",
    "            )\n",
    "            \"\"\"\n",
    "            stats = spark.sql(stats_sql).collect()[0]\n",
    "            \n",
    "            # Distribution stats (with sampling option)\n",
    "            dist_template = 'distribution_stats_sampled' if use_sampling else 'distribution_stats'\n",
    "            dist_query = QUERIES[dist_template].format(\n",
    "                column=safe_column, schema=safe_schema, table=safe_table\n",
    "            )\n",
    "            escaped_dist = dist_query.replace(\"'\", \"''\")\n",
    "            \n",
    "            dist_sql = f\"\"\"\n",
    "            SELECT * FROM remote_query(\n",
    "                '{connection_name}',\n",
    "                database => '{sql_database}',\n",
    "                query => '{escaped_dist}',\n",
    "                fetchsize => '10'\n",
    "            )\n",
    "            \"\"\"\n",
    "            dist = spark.sql(dist_sql).collect()[0]\n",
    "            \n",
    "        else:\n",
    "            # Federation approach\n",
    "            print(f\"Using federation approach for {column}\")\n",
    "            stats = spark.sql(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_rows,\n",
    "                    COUNT(DISTINCT {column}) as distinct_values,\n",
    "                    COUNT({column}) as non_null_count,\n",
    "                    MIN({column}) as min_value,\n",
    "                    MAX({column}) as max_value\n",
    "                FROM {src_catalog}.{src_schema}.{src_table}\n",
    "            \"\"\").collect()[0]\n",
    "            \n",
    "            if use_sampling:\n",
    "                # Sample-based distribution\n",
    "                dist = spark.sql(f\"\"\"\n",
    "                    WITH sampled AS (\n",
    "                        SELECT {column}\n",
    "                        FROM {src_catalog}.{src_schema}.{src_table} TABLESAMPLE (1000000 ROWS)\n",
    "                        WHERE {column} IS NOT NULL\n",
    "                    )\n",
    "                    SELECT \n",
    "                        AVG(CAST(cnt AS DOUBLE)) as avg_frequency,\n",
    "                        MAX(cnt) as max_frequency\n",
    "                    FROM (\n",
    "                        SELECT COUNT(*) as cnt\n",
    "                        FROM sampled\n",
    "                        GROUP BY {column}\n",
    "                    ) freq\n",
    "                \"\"\").collect()[0]\n",
    "            else:\n",
    "                # Full distribution\n",
    "                dist = spark.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                        AVG(CAST(cnt AS DOUBLE)) as avg_frequency,\n",
    "                        MAX(cnt) as max_frequency\n",
    "                    FROM (\n",
    "                        SELECT COUNT(*) as cnt\n",
    "                        FROM {src_catalog}.{src_schema}.{src_table}\n",
    "                        WHERE {column} IS NOT NULL\n",
    "                        GROUP BY {column}\n",
    "                    ) freq\n",
    "                \"\"\").collect()[0]\n",
    "        \n",
    "        # Always return Row objects\n",
    "        return stats, dist\n",
    "\n",
    "    # Profile each column - with fixed result processing\n",
    "    results = []\n",
    "    for i, column in enumerate(candidate_columns, 1):\n",
    "        print(f\"[{i}/{len(candidate_columns)}] Profiling {column}... \", end=\"\")\n",
    "        if use_sampling:\n",
    "            print(\"(sampling) \", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            stats, dist = profile_column(column)\n",
    "            \n",
    "            # Access Row fields directly - no .get() needed\n",
    "            total_rows = int(stats['total_rows'])\n",
    "            non_null_count = int(stats['non_null_count'])\n",
    "            distinct_values = int(stats['distinct_values'])\n",
    "            \n",
    "            # Check if distribution stats are available\n",
    "            avg_freq = dist['avg_frequency']\n",
    "            max_freq = dist['max_frequency']\n",
    "            \n",
    "            result = {\n",
    "                'column_name': column,\n",
    "                'total_rows': total_rows,\n",
    "                'distinct_values': distinct_values,\n",
    "                'null_percentage': round((total_rows - non_null_count) * 100.0 / total_rows, 2) if total_rows > 0 else 0,\n",
    "                'min_value': stats['min_value'],\n",
    "                'max_value': stats['max_value'],\n",
    "                'avg_rows_per_value': int(avg_freq) if avg_freq is not None else None,\n",
    "                'max_rows_per_value': int(max_freq) if max_freq is not None else None\n",
    "            }\n",
    "            \n",
    "            # Calculate and display skew if available\n",
    "            if avg_freq and max_freq and avg_freq > 0:\n",
    "                skew = max_freq / avg_freq\n",
    "                print(f\"✓ Skew: {skew:.1f}x\")\n",
    "            else:\n",
    "                print(\"✓ Stats collected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {str(e)[:100]}\")\n",
    "            result = {'column_name': column, 'error': str(e)}\n",
    "        \n",
    "        results.append(result)\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e44253-b0ec-476e-a2ff-10bc91b907f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Score and Rank Columns\n",
    "Score each column based on its suitability as a partition column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54c8b2c-a715-45df-9ae7-c29fa32d5f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Important Note: \n",
    "The skew ratio tiers (2x, 5x, 10x) are designed to catch problematic distribution patterns that can actually impact job performance:\n",
    "- <2x skew: Negligible impact - partition processing times stay balanced\n",
    "- 2-5x skew: Noticeable but manageable - some partitions take longer but parallelism still effective\n",
    "- 5-10x skew: Performance degradation - long-tail partitions start blocking executor slots\n",
    "- 10x skew: Severe impact - one partition could take longer than processing 10 average ones combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a28495-141a-4b4a-aa55-c7c5c294ab27",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764145591061}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate Columns Based on Skew and Nulls\n",
    "\n",
    "if not SKIP_PROFILING and not results_df.empty:\n",
    "    \n",
    "    def score_column(row):\n",
    "        \"\"\"Simple scoring based on skew and nulls - handles None values\"\"\"\n",
    "        if 'error' in row:\n",
    "            return -1\n",
    "        \n",
    "        # Skip if no distribution stats available\n",
    "        if row.get('avg_rows_per_value') is None or row.get('max_rows_per_value') is None:\n",
    "            return -2  # Can't score without distribution data\n",
    "        \n",
    "        null_pct = row['null_percentage']\n",
    "        avg_rows = row['avg_rows_per_value']\n",
    "        max_rows = row['max_rows_per_value']\n",
    "        \n",
    "        # Disqualify if >1% nulls\n",
    "        if null_pct > 1:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate skew\n",
    "        if avg_rows > 0:\n",
    "            skew_ratio = max_rows / avg_rows\n",
    "            \n",
    "            # Score based on skew\n",
    "            if skew_ratio > 10:\n",
    "                return 0  # Too skewed\n",
    "            elif skew_ratio > 5:\n",
    "                return 40  # High skew\n",
    "            elif skew_ratio > 2:\n",
    "                return 70  # Moderate skew\n",
    "            else:\n",
    "                return 100  # Low skew\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Apply scoring - handle None values properly\n",
    "    results_df['skew_ratio'] = results_df.apply(\n",
    "        lambda r: r['max_rows_per_value'] / r['avg_rows_per_value'] \n",
    "        if r.get('avg_rows_per_value') and r['avg_rows_per_value'] > 0 \n",
    "        else None, axis=1\n",
    "    )\n",
    "    results_df['score'] = results_df.apply(score_column, axis=1)\n",
    "    \n",
    "    # Sort by score\n",
    "    results_df = results_df.sort_values('score', ascending=False)\n",
    "    \n",
    "    # Display\n",
    "    display_cols = ['column_name', 'distinct_values', 'null_percentage', \n",
    "                   'skew_ratio', 'score']\n",
    "    display(results_df[[c for c in display_cols if c in results_df.columns]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b00afb5e-2ac4-4a09-bff9-82f27ed13320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Final Recommendations\n",
    "Generate configuration recommendations for the best partition column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9033549f-2032-4f69-8a97-0946b111efea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Recommendations\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARTITION COLUMN RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if SKIP_PROFILING:\n",
    "    # Already showed PK recommendation\n",
    "    pass\n",
    "elif not results_df.empty:\n",
    "    valid = results_df[results_df['score'] > 0]\n",
    "    \n",
    "    if not valid.empty:\n",
    "    # Find ALL columns with the top score\n",
    "        best_score = valid.iloc[0]['score']\n",
    "        top_columns = valid[valid['score'] == best_score]\n",
    "    \n",
    "        if len(top_columns) > 1:\n",
    "            print(f\"\\n MULTIPLE OPTIONS (Score: {best_score}/100)\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            for idx, row in top_columns.iterrows():\n",
    "                print(f\"\\nOption {idx+1}: {row['column_name']}\")\n",
    "                print(f\"  • Distinct values: {row['distinct_values']:,}\")\n",
    "                print(f\"  • Null %: {row['null_percentage']:.2f}%\")\n",
    "                if row.get('skew_ratio'):\n",
    "                    print(f\"  • Skew ratio: {row['skew_ratio']:.1f}x\")\n",
    "                \n",
    "                # Special indicators\n",
    "                if pk_column and row['column_name'] == pk_column:\n",
    "                    print(\"   CLUSTERED PRIMARY KEY - Recommended!\")\n",
    "                elif row['distinct_values'] == row['total_rows']:\n",
    "                    print(\"   Unique column - consider if truly sequential\")\n",
    "            \n",
    "            print(\"\\nRecommendation: Choose based on:\")\n",
    "            print(\"  1. Clustered PK (if available)\")\n",
    "            print(\"  2. Higher distinct values (more flexibility in partitioning)\")\n",
    "            print(\"  3. Business logic (predictable growth)\")\n",
    "            \n",
    "        else:\n",
    "            # Single best column\n",
    "            best = top_columns.iloc[0]\n",
    "            print(f\"\\n✅ RECOMMENDED: {best['column_name']}\")\n",
    "    else:\n",
    "        print(\"\\n No suitable partition columns found!\")\n",
    "        print(\"\\nRecommendations:\")\n",
    "        print(\"1. Add a clustered index on primary key (like, IDENTITY column) or if not PK, add a clustered index on a high cardinality column with no null values\")\n",
    "        print(\"2. Use non-partitioned ingestion if the dataset is less than 10GB\")\n",
    "        \n",
    "    \n",
    "    print(\"\\nConfiguration:\")\n",
    "    if not valid.empty:\n",
    "        print(f\"\"\"\n",
    "UPDATE control_table\n",
    "SET \n",
    "    partition_col = '{best['column_name']}',\n",
    "    partition_size_mb = 2048,\n",
    "    load_partitioned = true\n",
    "WHERE src_table = '{src_catalog}.{src_schema}.{src_table}';\n",
    "\"\"\")\n",
    "    else:\n",
    "        print(f\"\"\"\n",
    "UPDATE control_table\n",
    "SET \n",
    "    load_partitioned = false  -- No suitable partition column\n",
    "WHERE src_table = '{src_catalog}.{src_schema}.{src_table}';\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTE: Clustered primary keys are ideal partition columns.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "table_profiler_sqlserver",
   "widgets": {
    "connection_name": {
     "currentValue": "sqlserver_edwia",
     "nuid": "340768eb-6e60-4ac6-85a6-f215d8f7df0d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Connection Name (for remote_query - leave empty to use federation)",
      "name": "connection_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Connection Name (for remote_query - leave empty to use federation)",
      "name": "connection_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "sql_database": {
     "currentValue": "DatabricksTestDB",
     "nuid": "65162289-356f-4952-a8d9-8284590d70bc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "SQL Server Database (for remote_query)",
      "name": "sql_database",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "SQL Server Database (for remote_query)",
      "name": "sql_database",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_catalog": {
     "currentValue": "sqlserver_edwia_catalog",
     "nuid": "361315fc-07cc-4324-a133-991dd63e357e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sqlserver_catalog",
      "label": "UC Foreign Catalog",
      "name": "src_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sqlserver_catalog",
      "label": "UC Foreign Catalog",
      "name": "src_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_schema": {
     "currentValue": "dbo",
     "nuid": "ab2bdb88-8ab5-4e92-81f4-d3db0ccf5b7b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dbo",
      "label": "Source Schema",
      "name": "src_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dbo",
      "label": "Source Schema",
      "name": "src_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "src_table": {
     "currentValue": "test_write",
     "nuid": "b7fa467f-39ad-44a2-9035-23e8c95634ad",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "store_sales_1tb",
      "label": "Source Table (REQUIRED)",
      "name": "src_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "store_sales_1tb",
      "label": "Source Table (REQUIRED)",
      "name": "src_table",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "use_sampling": {
     "currentValue": "false",
     "nuid": "fd6ea3e7-5a0f-4a73-a9c2-66052f618960",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Use Sampling (for 10TB+ tables)",
      "name": "use_sampling",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Use Sampling (for 10TB+ tables)",
      "name": "use_sampling",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}